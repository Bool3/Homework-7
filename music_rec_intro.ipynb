{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Towards a Recommendation Engine\n",
    "\n",
    "Netflix and Google and Amazon and Reddit and TikTok and ... are all companies that have build their core or significant portions of their business around their recommendation engines. There are differences in the way that these engines are deployed by companies and of course their implementations no doubt vary widely, but at their core they are all based on the idea of making suggestions (facilitating discovery) based on conclusions drawn from data collected about people. This year I have been collecting data about you and my other student's music preferences. In this unit we are going to build a recommendation engine based on that collected data and see how well we can make suggestions to your classmates.\n",
    "\n",
    "## Collective Intelligence\n",
    "\n",
    "Collective Intelligence is the idea that data collected from a wide range of people in a domain to get insisghts into questions and ideas on how a system, generally complex, operates. This is not a recent phenomena - this is the field of population statistics (e.g. the census) - but the internet has provided an easily accessible sandbox for collecting this type of data in a wide range of interesting activity. Another interesting example are the Futures Markets. These are markets where individuals buy and sell contracts for the future purchase and sale of some item. The prices for these contracts, futures, are set based on the forces of the free market and combine the beliefs, opinions, facts, and knowledge of all of the market participants. If the markets are large enough these systems can \"project\" outcomes better than experts or expert systems. Or so goes the theory (see the United State's presidential election 2016, 2020 and the United States Congressional races 2020.)  \n",
    "\n",
    "Crowd Sourcing is another common term related to collective intelligence. The idea behind crowd sourcing is to tap the wisdom of the crowds to produce something. Crowd sourcing relies on collective intelligence. Wikipedia and Google are 2 of the most widely used crowd sourced resources in culture. Reddit is crowd sourced news. And Kickstarter is crowd sourced business funding. \n",
    "\n",
    "### Interesting Sites\n",
    "\n",
    "* [Five Thirty Eight](http://fivethirtyeight.com )  \n",
    "* [Inkling Markets](http://inklingmarkets.com )  \n",
    "* [Hollywood Stock Exchange](http://www.hsx.com )  \n",
    "* [Threadless](http://threadless.com)\n",
    "* [KickStarter](http://kickstarter.com)\n",
    "* [All Sides](http://allsides.com)\n",
    "\n",
    "## Machine Learning vs. AI\n",
    "\n",
    "AI, that we worked on during the beginning of the year, is characterized by working on how we can codify reasoning and logic to help computers (and us) make better decisions. I think of AI as being in the provenance of reasoning and logic. Given a set of circumstances and potentially some prior knowledge about a domain what is the best thing that I can do right now to make myself the most satisfied.\n",
    "\n",
    "Machine learning on the other hand is concerned with computers (and therefore us) designing systems that can change their decision making process based on observation of past outcomes. In other words, can machines improve their outcomes by looking at lots of examples of prior outcomes given a set of circumstances. This should sound annoyingly familiar. How often have adults used their experience to impress behavioral changes upon you? With machine learnign we use collected data and their related outcomes as \"experiences\" that we feed to a variety of mathematical and statistical algorithms to find patterns (define functions) that we can then use to make predictions about what will happen given unseen inputs.\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Collaborative Filtering\n",
    "\n",
    "Collaborative Filtering is a term that was introduced in 1992 by David Goldberg [\"Using Collaborative Filtering to weave an information tapestry\"](http://www.bitsavers.org/pdf/xerox/parc/techReports/CSL-92-10_Using_Collaborative_Filtering_to_Weave_an_Information_Tapestry.pdf). The basic idea behind collaborative filtering is to use the recommendations of a large group of people to find a small group that has tastes similar to yours. Once this group is identified then the items that they like that you have not experienced are the items that should be given priority in a search of new things to show you. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Step 1: Collecting & Preparing Data\n",
    "=======================\n",
    "\n",
    "We have already done this during the first term. In order to give you a feel for the entire process we will go through an abbreviated data cleaning and preparation phase. This is a critical step and in any project you will spend more time preparing data than you would initially have thought was required.  \n",
    "\n",
    "To get a sense of the data that we have collected open the data file in Google Spreadsheet. There are several ways that you can visualize raw data, but I find spreadsheets to be a quick, easy, and comprhensive way to give you a general feel for the size and shape of the data. You can also use this as an opportunity to make a note of potential problems that you might incur during the data preparation process. For example, missing data, badly formatted data, etc.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## CSV Files\n",
    "\n",
    "We are not going to work with the data in the spreadsheet, although that is possible. Instead we are going to download the data in comma separated value form so that we can process it in code and begin working with it through code.\n",
    "\n",
    "Working with CSV is very easy in Python. There is a module ```csv``` that provides tools for parsing csv into Python data structures. If you want to read more about the ```csv``` module check out [Real Python](https://realpython.com/python-csv/).  \n",
    "\n",
    "For this project we are going to use the ```DictReader``` method to parse the csv file into a dictionary.  The format that we want our internal dictionary to be in is:\n",
    "\n",
    "```\n",
    "# { studentId: {songName: rating, songName : rating, ...}, ... }\n",
    "\n",
    "# Ex. \n",
    "\n",
    "test_data= \\\n",
    "{'s001': {'Kiss': 2.5, 'Mozart': 3.5, 'Snoop': 3.0, 'Radiohead': 3.5, 'Led Zeppelin': 2.5, 'Flying Lotus': 3.0},\n",
    "'s002': {'Kiss': 3.0, 'Mozart': 3.5, 'Snoop': 1.5, 'Radiohead': 5.0, 'Flying Lotus': 3.0, 'Led Zeppelin': 3.5}}\n",
    "\n",
    "# accessing the dictionary then works like this:\n",
    "\n",
    "assert test_data['s001']['Mozart'] == 3.5\n",
    "```\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Step 2: Calculating Similarity\n",
    "==================\n",
    "\n",
    "How similar are 2 students? How can we use the ratings to measure similarity? We will evaluate 2 different measures. As with most things in AI the best tool for the job is going to depend on the problem. Each algorithm has strengths and weaknesses that may show themselves in unpredictable ways when applied to different problems. Try different approaches.\n",
    "\n",
    "Metric 1: Euclidean Distance\n",
    "----------------------------\n",
    "This is the distance formula that you are used to from geometry. However, we will apply the formula to higher dimensions than 3. Given 2 points, _x_  and _y_ in _n_ dimension, (think of dimensions as different measurements of the point), the formula is:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left(\\sum _{i=1}^{n}\\left|x_{i}-y_{i}\\right|^{2}\\right)^{1/2}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "Metric 2: Pearson Correlation Coefficient\n",
    "----------------------------\n",
    "\n",
    "The Pearson Correlation Coefficient measures how correlated 2 variables are with one another. The value ranges from _-1_, perfectly inversely related to, _+1_, perfectly correlated. _0_ indicates no correlation between the variables. For us, PCC will measure how correlated 2 reviewers are. The nice thing about the pearson coefficient is that it is not sensitive to differences in the rankers tendencies. For instance, 2 reviewers can be highly correlated even if they have absolute differences between their scores.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{r =} \\frac{ \\sum  \\text{XY} - \\frac{\\sum \\text{X} \\sum \\text{Y}}{\\text{N}} } { \\sqrt{ \\left( \\sum X^{2} - \\frac {\\left( \\sum \\text{X} \\right)^{2} } {N} \\right)} \\left( \\sum Y^{2} - \\frac {\\left( \\sum \\text{Y} \\right)^{2} } {N} \\right) }  \n",
    "\\end{equation*}\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}